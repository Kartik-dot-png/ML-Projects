# -*- coding: utf-8 -*-
"""Financial Risk Prediction Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O2Ic_q81ymgjvhs0r-IvMeTto5MTmaqe
"""

# FInancial Sentiment Analysis

import os
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
import warnings

# Machine Learning Imports
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score
from gensim.models import Word2Vec
from tqdm import tqdm

# Deep Learning Imports
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import evaluate

# --- Global Configuration & Setup
warnings.filterwarnings('ignore')
tqdm.pandas()

# Configuration variables
PROCESSED_DATA_FILE = 'processed_it_sector_data.csv'
TARGET_SECTOR = 'Information Technology'
RANDOM_STATE = 42


#PHASE 1: DATA ENGINEERING
def get_sp500_it_tickers():
    """Scrapes Wikipedia to get a list of S&P 500 tickers for the IT sector."""
    print("Fetching S&P 500 IT sector tickers from Wikipedia...")
    url = 'http://en.wikipedia.org/wiki/List_of_S%26P_500_companies'
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    table = soup.find('table', {'class': 'wikitable sortable'})

    tickers = []
    for row in table.findAll('tr')[1:]:
        sector = row.findAll('td')[2].text.strip()
        if sector == TARGET_SECTOR:
            ticker = row.findAll('td')[0].text.strip()
            tickers.append(ticker)

    print(f"Found {len(tickers)} tickers in the '{TARGET_SECTOR}' sector.")
    return tickers

def run_phase1_data_engineering():
    """
    Loads raw data, filters for the IT sector, engineers the volatility
    target label, and saves a processed CSV file.
    """
    print("\n--- Starting Phase 1: Data Engineering ---")
    if os.path.exists(PROCESSED_DATA_FILE):
        print(f"'{PROCESSED_DATA_FILE}' already exists. Skipping Phase 1.")
        return

    import yfinance as yf
    from datasets import load_dataset

    it_tickers = get_sp500_it_tickers()
    if not it_tickers:
        print("Could not fetch tickers, using a fallback list.")
        it_tickers = ['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'ORCL', 'ADBE', 'CRM', 'INTC', 'CSCO']

    print("Step 1.1: Loading earnings call data from Hugging Face...")
    dataset = load_dataset("jakob-k/earnings-calls-sp-500", split="train")
    df = dataset.to_pandas()
    df['date'] = pd.to_datetime(df['date'])
    df_it = df[df['ticker'].isin(it_tickers)].copy()
    print(f"Filtered down to {len(df_it)} transcripts for the IT sector.")

    def get_volatility(ticker, call_date):
        try:
            start_date = call_date + pd.Timedelta(days=1)
            end_date = start_date + pd.Timedelta(days=7)
            stock_data = yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=True)
            if stock_data.empty or len(stock_data) < 2: return None
            return stock_data['Close'].pct_change().std()
        except Exception:
            return None

    print("Step 1.2: Programmatically calculating post-call volatility...")
    df_it['volatility'] = df_it.progress_apply(lambda r: get_volatility(r['ticker'], r['date']), axis=1)
    df_it.dropna(subset=['volatility', 'transcript'], inplace=True)

    # Clean transcript text
    df_it['transcript'] = df_it['transcript'].apply(lambda x: re.sub(r'\s+', ' ', x).strip())

    print("Step 1.3: Creating binary risk label...")
    high_volatility_threshold = df_it['volatility'].quantile(0.75)
    df_it['is_high_risk'] = (df_it['volatility'] > high_volatility_threshold).astype(int)

    final_df = df_it[['ticker', 'date', 'transcript', 'is_high_risk']]
    final_df.to_csv(PROCESSED_DATA_FILE, index=False)
    print(f"Phase 1 Complete. Saved {len(final_df)} processed records to '{PROCESSED_DATA_FILE}'.")
    print("Label distribution:\n", final_df['is_high_risk'].value_counts(normalize=True))


#PHASE 2: CLASSIC ML BASELINES
def run_phase2_classic_ml():
    """Trains and evaluates classic ML models on both TF-IDF and Word2Vec features."""
    print("\n--- Starting Phase 2: Classic Machine Learning Baselines ---")
    df = pd.read_csv(PROCESSED_DATA_FILE)
    X_text = df['transcript']
    y = df['is_high_risk']
    X_train_text, X_test_text, y_train, y_test = train_test_split(X_text, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y)

    models = {
        "Naive Bayes": MultinomialNB(),
        "SVM": LinearSVC(random_state=RANDOM_STATE),
        "Random Forest": RandomForestClassifier(random_state=RANDOM_STATE),
        "XGBoost": XGBClassifier(random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss')
    }

    # Track A: TF-IDF Features
    print("\n--- Track 2A: Training on TF-IDF Features ---")
    tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1, 2))
    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)
    X_test_tfidf = tfidf_vectorizer.transform(X_test_text)

    for name, model in models.items():
        print(f"\n--- Evaluating {name} with TF-IDF ---")
        model.fit(X_train_tfidf, y_train)
        y_pred = model.predict(X_test_tfidf)
        print(classification_report(y_test, y_pred, target_names=['Low Risk', 'High Risk']))

    # Track B: Word2Vec Features
    print("\n--- Track 2B: Training on Word2Vec Features ---")
    # Preprocess text for Word2Vec (list of lists of tokens)
    X_train_tok = [re.findall(r'\b\w+\b', doc.lower()) for doc in X_train_text]
    X_test_tok = [re.findall(r'\b\w+\b', doc.lower()) for doc in X_test_text]

    w2v_model = Word2Vec(sentences=X_train_tok, vector_size=300, window=5, min_count=5, workers=4)
    word_vectors = w2v_model.wv

    def create_doc_vector(tokens, model):
        vectors = [model[word] for word in tokens if word in model]
        if not vectors:
            return np.zeros(model.vector_size)
        return np.mean(vectors, axis=0)

    X_train_w2v = np.array([create_doc_vector(tokens, word_vectors) for tokens in X_train_tok])
    X_test_w2v = np.array([create_doc_vector(tokens, word_vectors) for tokens in X_test_tok])

    #######  Naive Bayes doesn't work with negative values from Word2Vec, so we skip it.
    for name, model in models.items():
        if name == "Naive Bayes": continue
        print(f"\n--- Evaluating {name} with Word2Vec ---")
        model.fit(X_train_w2v, y_train)
        y_pred = model.predict(X_test_w2v)
        print(classification_report(y_test, y_pred, target_names=['Low Risk', 'High Risk']))

    print("Phase 2 Complete.")


#PHASE 3: SEQUENTIAL DEEP LEARNING (BI-LSTM)

def run_phase3_bilstm():
    """Builds, trains, and evaluates a Bi-LSTM model."""
    print("\n--- Starting Phase 3: Sequential Deep Learning (Bi-LSTM) ---")
    df = pd.read_csv(PROCESSED_DATA_FILE)
    X_text = df['transcript'].astype(str)
    y = df['is_high_risk'].values
    X_train_text, X_test_text, y_train, y_test = train_test_split(X_text, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y)

    # Tokenization and Padding
    vocab_size = 10000
    max_length = 512
    trunc_type = 'post'
    oov_tok = "<OOV>"

    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
    tokenizer.fit_on_texts(X_train_text)

    X_train_seq = tokenizer.texts_to_sequences(X_train_text)
    X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, truncating=trunc_type)

    X_test_seq = tokenizer.texts_to_sequences(X_test_text)
    X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, truncating=trunc_type)

    # Model Building
    embedding_dim = 128
    model = Sequential([
        Embedding(vocab_size, embedding_dim, input_length=max_length),
        Bidirectional(LSTM(64, return_sequences=True)),
        Dropout(0.5),
        Bidirectional(LSTM(32)),
        Dropout(0.5),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')
    ])

    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()

    print("\nTraining Bi-LSTM model...")
    model.fit(X_train_pad, y_train, epochs=3, batch_size=32, validation_split=0.1, verbose=2)

    print("\nEvaluating Bi-LSTM model...")
    y_pred_proba = model.predict(X_test_pad)
    y_pred = (y_pred_proba > 0.5).astype(int)
    print(classification_report(y_test, y_pred, target_names=['Low Risk', 'High Risk']))
    print(f"Bi-LSTM AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")
    print("Phase 3 Complete.")

#PHASE 4: TRANSFORMER DEEP LEARNING (ROBERTA)
def run_phase4_roberta():
    """Fine-tunes a RoBERTa model for end-to-end classification."""
    print("\n--- Starting Phase 4: Transformer Deep Learning (RoBERTa) ---")
    df = pd.read_csv(PROCESSED_DATA_FILE).dropna(subset=['transcript'])
    df = df.rename(columns={'is_high_risk': 'label'})

    # Use a smaller subset for faster local training
    num_samples = min(df['label'].value_counts().min(), 250)
    df_balanced = df.groupby('label').apply(lambda x: x.sample(n=num_samples, random_state=RANDOM_STATE)).reset_index(drop=True)

    print(f"Step 4.1: Preparing a balanced dataset of {len(df_balanced)} samples...")
    dataset = Dataset.from_pandas(df_balanced)
    hf_datasets = dataset.train_test_split(test_size=0.25, stratify_by_column="label")

    model_name = "roberta-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    def tokenize_function(examples):
        return tokenizer(examples["transcript"], padding="max_length", truncation=True, max_length=512)

    tokenized_datasets = hf_datasets.map(tokenize_function, batched=True)

    print("Step 4.2: Setting up Trainer...")
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

    f1_metric = evaluate.load("f1")
    auc_metric = evaluate.load("roc_auc")

    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        f1 = f1_metric.compute(predictions=predictions, references=labels, average="weighted")
        auc = auc_metric.compute(prediction_scores=logits[:, 1], references=labels)
        return {"f1": f1["f1"], "roc_auc": auc["roc_auc"]}

    training_args = TrainingArguments(
        output_dir="roberta_risk_trainer",
        evaluation_strategy="epoch",
        num_train_epochs=2, # good satarting point
        per_device_train_batch_size=8, # based on GPU
        per_device_eval_batch_size=8,
        logging_steps=50,
        save_strategy="epoch",
        load_best_model_at_end=True,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        compute_metrics=compute_metrics,
    )

    print("Step 4.3: Fine-tuning RoBERTa model...")
    trainer.train()

    print("\nStep 4.4: Evaluating RoBERTa model...")
    eval_results = trainer.evaluate()
    print(f"Final RoBERTa Evaluation Results: {eval_results}")
    print("Phase 4 Complete.")

#MAIN EXECUTION
if __name__ == '__main__':
    run_phase1_data_engineering()
    run_phase2_classic_ml()
    run_phase3_bilstm()
    run_phase4_roberta()
    print("\n--- Project Execution Finished ---")